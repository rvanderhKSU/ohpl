{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Metrics to assess performance on ordinal classification task given class prediction\n",
    "   using hyper plane loss techniques \n",
    "\"\"\"\n",
    "\n",
    "# Authors: Bob Vanderheyden <rvanderh@us.ibm.com>\n",
    "#          Ying Xie <yxie2@kennesaw.edu>\n",
    "#         \n",
    "# Contributor: Shayan Shamskolahi\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def hpall_mean_loss(y_true, y_pred, minlabel, maxlabel, margin=0.1, ordering_loss_weight=1):\n",
    "    \"\"\" Evaluate the ordinal hyperplane ordering loss and point loss of the predictions y_pred\\\n",
    "        (using reduce mean).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : array-like\n",
    "        y_pred : array-like\n",
    "        minlabel : integer\n",
    "        maxlabel : integer\n",
    "        margin : float\n",
    "        ordering_loss_weight : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "        A non-negative floating point value (best value is 0.0)\n",
    "        \n",
    "        Usage\n",
    "        -------\n",
    "        loss = hp_all_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "        print('Loss: ', loss.numpy()) # Loss: 0.7228571\n",
    "        \n",
    "        \n",
    "        Usage with the `compile` API:\n",
    "        \n",
    "        ```python\n",
    "        \n",
    "        Example Keras wrapper for hp_all_loss:\n",
    "        \n",
    "        def get_ohpl_wrapper (min_label, max_label, margin, ordering_loss_weight):\n",
    "            def ohpl(y_true, y_pred):\n",
    "                return hpall_mean_loss(y_true, y_pred, min_label, max_label, margin, ordering_loss_weight)\n",
    "            return ohpl\n",
    "\n",
    "        loss = get_ohpl_wrapper(2,7,.3,1) # ordering_loss_weight must not be less that 1\n",
    "        \n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        model.compile(loss=hp_all_loss, optimizer='adam', loss=ohpl_point_loss)\n",
    "        ```\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    min_label = tf.constant(minlabel, dtype=tf.float32)\n",
    "    max_label = tf.constant(maxlabel, dtype=tf.float32)\n",
    "    margin = tf.constant(margin, dtype=tf.float32) # centroid margin\n",
    "    ordering_loss_weight = tf.constant(ordering_loss_weight, dtype=tf.float32) \n",
    "    \n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.dtypes.cast(y_true, y_pred.dtype)\n",
    "    y_pred = tf.reshape(tf.transpose(y_pred),[-1,1])\n",
    "    \n",
    "    # OHPL ordering loss\n",
    "    # one hot vector for y_true\n",
    "    ords, idx = tf.unique(tf.reshape(y_true, [-1])) \n",
    "    num = tf.shape(ords)[0]\n",
    "    y_true_1hot = tf.one_hot(idx, num)\n",
    "\n",
    "    # mean distance for each class\n",
    "    yO = tf.matmul(tf.transpose(y_pred),y_true_1hot)\n",
    "    yc = tf.reduce_sum(y_true_1hot,0)\n",
    "    class_mean = tf.divide(yO,yc)  \n",
    "\n",
    "    # min. distance\n",
    "    ords = tf.dtypes.cast(ords, tf.float32)\n",
    "    ords0 = tf.reshape(ords, [-1,1])\n",
    "    ords1 = tf.reshape(ords, [1,-1])\n",
    "    \n",
    "    min_distance = tf.subtract(ords0, ords1)\n",
    "    # apply ReLU\n",
    "    min_distance = tf.nn.relu (min_distance)\n",
    "    \n",
    "    # keeps min. distance\n",
    "    keep = tf.minimum(min_distance,1)\n",
    "\n",
    "    # distance to centroid     \n",
    "    class_mean0 = tf.reshape(class_mean, [-1,1])\n",
    "    class_mean1 = tf.reshape(class_mean, [1,-1])\n",
    "    class_mean = tf.subtract(class_mean0, class_mean1)  \n",
    "    # apply ReLU    \n",
    "    class_mean = tf.nn.relu(class_mean)\n",
    "    centroid_distance = tf.multiply(keep, class_mean)\n",
    "    \n",
    "    hp_ordering_loss = tf.subtract(min_distance,centroid_distance)\n",
    "    # apply ReLU\n",
    "    hp_ordering_loss = tf.nn.relu(hp_ordering_loss)\n",
    "    hp_ordering_loss = tf.reduce_sum(hp_ordering_loss)\n",
    "    \n",
    "    # OHPL point loss\n",
    "    # Centroid for point\n",
    "    point_cent = tf.matmul(y_true_1hot, class_mean0)\n",
    "    \n",
    "    lower_bound = tf.subtract(min_label,y_true)\n",
    "    lower_bound = tf.add(lower_bound,1)\n",
    "    lower_bound = tf.multiply(lower_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    lower_bound = tf.nn.relu(lower_bound)\n",
    "    lower_bound = tf.add(margin, lower_bound)\n",
    "\n",
    "    upper_bound = tf.subtract(y_true,max_label)\n",
    "    upper_bound = tf.add(upper_bound,1)\n",
    "    upper_bound = tf.multiply(upper_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    upper_bound = tf.nn.relu(upper_bound)\n",
    "    upper_bound = tf.add(margin, upper_bound)    \n",
    "\n",
    "    upper_loss = tf.add(point_cent,upper_bound[:,tf.newaxis])\n",
    "    upper_loss = tf.subtract(y_pred,upper_loss)\n",
    "    # apply ReLU    \n",
    "    upper_loss = tf.nn.relu(upper_loss)\n",
    "    \n",
    "    lower_loss = tf.add(lower_bound[:,tf.newaxis],y_pred)\n",
    "    lower_loss = tf.subtract(point_cent,lower_loss)\n",
    "    # apply ReLU    \n",
    "    lower_loss = tf.nn.relu(lower_loss)\n",
    "   \n",
    "    hp_point_loss = tf.add(upper_loss, lower_loss)\n",
    "    hp_point_loss = tf.reduce_mean(hp_point_loss)\n",
    "\n",
    "    # aggregate ordering loss and point loss     \n",
    "    mean_loss = tf.add(hp_point_loss,tf.multiply(ordering_loss_weight, hp_ordering_loss))\n",
    "    \n",
    "    return mean_loss\n",
    "\n",
    "   \n",
    "    \"\"\"    \n",
    "        References\n",
    "        ----------\n",
    "        .. [1] Vanderheyden, Bob and Ying Xie. Ordinal Hyperplane Loss. (2018). \n",
    "           2018 IEEE International Conference on Big Data (Big Data), \n",
    "           2018 IEEE International Conference On, 2337. https://doi-org.proxy.kennesaw.edu/10.1109/BigData.2018.8622079\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpall_sum_loss(y_true, y_pred, minlabel, maxlabel, margin=0.1, ordering_loss_weight=1):\n",
    "    \"\"\" Evaluate the ordinal hyperplane ordering loss and point loss of the predictions y_pred\\\n",
    "        (using reduce sum).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : array-like\n",
    "        y_pred : array-like\n",
    "        minlabel : integer\n",
    "        maxlabel : integer\n",
    "        margin : float\n",
    "        ordering_loss_weight : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "        A non-negative floating point value (best value is 0.0)\n",
    "        \n",
    "        Usage\n",
    "        -------\n",
    "        loss = hp_all_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "        print('Loss: ', loss.numpy()) # Loss: 3.48\n",
    "        \n",
    "        \n",
    "        Usage with the `compile` API:\n",
    "        \n",
    "        ```python\n",
    "        \n",
    "        Example Keras wrapper for hp_all_loss:\n",
    "        \n",
    "        def get_ohpl_wrapper (min_label, max_label, margin, ordering_loss_weight):\n",
    "            def ohpl(y_true, y_pred):\n",
    "                return hpall_sum_loss(y_true, y_pred, min_label, max_label, margin, ordering_loss_weight)\n",
    "            return ohpl\n",
    "\n",
    "        loss = get_ohpl_wrapper(0,4,1,1)\n",
    "        \n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        model.compile(loss=hp_all_loss, optimizer='adam', loss=ohpl_point_loss)\n",
    "        ```\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    min_label = tf.constant(minlabel, dtype=tf.float32)\n",
    "    max_label = tf.constant(maxlabel, dtype=tf.float32)\n",
    "    margin = tf.constant(margin, dtype=tf.float32) # centroid margin\n",
    "    ordering_loss_weight = tf.constant(ordering_loss_weight, dtype=tf.float32) \n",
    "    \n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.dtypes.cast(y_true, y_pred.dtype)\n",
    "    y_pred = tf.reshape(tf.transpose(y_pred),[-1,1])\n",
    "    \n",
    "    # OHPL ordering loss\n",
    "    # one hot vector for y_true\n",
    "    ords, idx = tf.unique(tf.reshape(y_true, [-1])) \n",
    "    num = tf.shape(ords)[0]\n",
    "    y_true_1hot = tf.one_hot(idx, num)\n",
    "\n",
    "    # mean distance for each class\n",
    "    yO = tf.matmul(tf.transpose(y_pred),y_true_1hot)\n",
    "    yc = tf.reduce_sum(y_true_1hot,0)\n",
    "    class_mean = tf.divide(yO,yc)  \n",
    "\n",
    "    # min. distance\n",
    "    ords = tf.dtypes.cast(ords, tf.float32)\n",
    "    ords0 = tf.reshape(ords, [-1,1])\n",
    "    ords1 = tf.reshape(ords, [1,-1])\n",
    "    \n",
    "    min_distance = tf.subtract(ords0, ords1)\n",
    "    # apply ReLU\n",
    "    min_distance = tf.nn.relu (min_distance)\n",
    "    \n",
    "    # keeps min. distance\n",
    "    keep = tf.minimum(min_distance,1)\n",
    "\n",
    "    # distance to centroid     \n",
    "    class_mean0 = tf.reshape(class_mean, [-1,1])\n",
    "    class_mean1 = tf.reshape(class_mean, [1,-1])\n",
    "    class_mean = tf.subtract(class_mean0, class_mean1)  \n",
    "    # apply ReLU    \n",
    "    class_mean = tf.nn.relu(class_mean)\n",
    "    centroid_distance = tf.multiply(keep, class_mean)\n",
    "    \n",
    "    hp_ordering_loss = tf.subtract(min_distance,centroid_distance)\n",
    "    # apply ReLU\n",
    "    hp_ordering_loss = tf.nn.relu(hp_ordering_loss)\n",
    "    hp_ordering_loss = tf.reduce_sum(hp_ordering_loss)\n",
    "    \n",
    "    # OHPL point loss\n",
    "    # Centroid for point\n",
    "    point_cent = tf.matmul(y_true_1hot, class_mean0)\n",
    "    \n",
    "    lower_bound = tf.subtract(min_label,y_true)\n",
    "    lower_bound = tf.add(lower_bound,1)\n",
    "    lower_bound = tf.multiply(lower_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    lower_bound = tf.nn.relu(lower_bound)\n",
    "    lower_bound = tf.add(margin, lower_bound)\n",
    "\n",
    "    upper_bound = tf.subtract(y_true,max_label)\n",
    "    upper_bound = tf.add(upper_bound,1)\n",
    "    upper_bound = tf.multiply(upper_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    upper_bound = tf.nn.relu(upper_bound)\n",
    "    upper_bound = tf.add(margin, upper_bound)    \n",
    "\n",
    "    upper_loss = tf.add(point_cent,upper_bound[:,tf.newaxis])\n",
    "    upper_loss = tf.subtract(y_pred,upper_loss)\n",
    "    # apply ReLU    \n",
    "    upper_loss = tf.nn.relu(upper_loss)\n",
    "    \n",
    "    lower_loss = tf.add(lower_bound[:,tf.newaxis],y_pred)\n",
    "    lower_loss = tf.subtract(point_cent,lower_loss)\n",
    "    # apply ReLU    \n",
    "    lower_loss = tf.nn.relu(lower_loss)\n",
    "   \n",
    "    hp_point_loss = tf.add(upper_loss, lower_loss)\n",
    "    hp_point_loss = tf.reduce_sum(hp_point_loss)\n",
    "\n",
    "    # aggregate ordering loss and point loss     \n",
    "    sum_loss = tf.add(hp_point_loss,tf.multiply(ordering_loss_weight, hp_ordering_loss))\n",
    "    \n",
    "    return sum_loss\n",
    "\n",
    "\n",
    "    \"\"\"    \n",
    "        References\n",
    "        ----------\n",
    "        .. [1] Vanderheyden, Bob and Ying Xie. Ordinal Hyperplane Loss. (2018). \n",
    "           2018 IEEE International Conference on Big Data (Big Data), \n",
    "           2018 IEEE International Conference On, 2337. https://doi-org.proxy.kennesaw.edu/10.1109/BigData.2018.8622079\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.7228571\n"
     ]
    }
   ],
   "source": [
    "loss = hpall_mean_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "print('Loss: ', loss.numpy()) # Loss: 0.7228571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  3.48\n"
     ]
    }
   ],
   "source": [
    "loss = hpall_sum_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "print('Loss: ', loss.numpy()) # Loss: 3.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application in Keras (mean loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 781 entries, 0 to 781\n",
      "Data columns (total 7 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Score                         781 non-null    int32  \n",
      " 1   GDP per capita                781 non-null    float64\n",
      " 2   Social support                781 non-null    float64\n",
      " 3   Healthy life expectancy       781 non-null    float64\n",
      " 4   Freedom to make life choices  781 non-null    float64\n",
      " 5   Generosity                    781 non-null    float64\n",
      " 6   Perceptions of corruption     781 non-null    float64\n",
      "dtypes: float64(6), int32(1)\n",
      "memory usage: 45.8 KB\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "pwd = !pwd\n",
    "df = pd.read_csv('world_happiness_2015_2019.csv')\n",
    "df.Score = df.Score.astype('int32')\n",
    "df.drop(['Year'], axis=1, inplace=True)\n",
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:]\n",
    "y = df.iloc[:,0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 3, 4, 5, 6, 7], dtype=int32), array([  7,  89, 202, 249, 162,  72]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper in action - Keras sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example Keras wrapper for hpall_mean_loss\n",
    "\n",
    "def get_ohpl_wrapper (min_label, max_label, margin, ordering_loss_weight):\n",
    "    def ohpl(y_true, y_pred):\n",
    "        return hpall_mean_loss(y_true, y_pred, min_label, max_label, margin, ordering_loss_weight)\n",
    "    return ohpl\n",
    "\n",
    "loss = get_ohpl_wrapper(2,7,1,1) # ordering_loss_weight must not be less that 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 523 samples\n",
      "Epoch 1/50\n",
      "523/523 [==============================] - 2s 3ms/sample - loss: 6.5075\n",
      "Epoch 2/50\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 3.9576\n",
      "Epoch 3/50\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 2.3436\n",
      "Epoch 4/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.8600\n",
      "Epoch 5/50\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.8316\n",
      "Epoch 6/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.2554\n",
      "Epoch 7/50\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 1.3583\n",
      "Epoch 8/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.5289\n",
      "Epoch 9/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.2710\n",
      "Epoch 10/50\n",
      "523/523 [==============================] - 1s 3ms/sample - loss: 1.1167\n",
      "Epoch 11/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.2297\n",
      "Epoch 12/50\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.9741\n",
      "Epoch 13/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9374\n",
      "Epoch 14/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7977\n",
      "Epoch 15/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9452\n",
      "Epoch 16/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0899\n",
      "Epoch 17/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8771\n",
      "Epoch 18/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 1.0177\n",
      "Epoch 19/50\n",
      "523/523 [==============================] - 1s 1ms/sample - loss: 0.7840\n",
      "Epoch 20/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8831\n",
      "Epoch 21/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7247\n",
      "Epoch 22/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8360\n",
      "Epoch 23/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8709\n",
      "Epoch 24/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.6345\n",
      "Epoch 25/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7453\n",
      "Epoch 26/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8109\n",
      "Epoch 27/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7673\n",
      "Epoch 28/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7414\n",
      "Epoch 29/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9718\n",
      "Epoch 30/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.9146\n",
      "Epoch 31/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8131\n",
      "Epoch 32/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7293\n",
      "Epoch 33/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8477\n",
      "Epoch 34/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.6795\n",
      "Epoch 35/50\n",
      "523/523 [==============================] - 1s 3ms/sample - loss: 0.8129\n",
      "Epoch 36/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7623\n",
      "Epoch 37/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7780\n",
      "Epoch 38/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7592\n",
      "Epoch 39/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7663\n",
      "Epoch 40/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7189\n",
      "Epoch 41/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8103\n",
      "Epoch 42/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7740\n",
      "Epoch 43/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7433\n",
      "Epoch 44/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7670\n",
      "Epoch 45/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7139\n",
      "Epoch 46/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7973\n",
      "Epoch 47/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.8049\n",
      "Epoch 48/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.6366\n",
      "Epoch 49/50\n",
      "523/523 [==============================] - 1s 2ms/sample - loss: 0.7402\n",
      "Epoch 50/50\n",
      "523/523 [==============================] - 1s 3ms/sample - loss: 0.7625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fccc51eb210>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and compile the model \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(25, activation='relu', input_shape=(6, )))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=loss, optimizer=\"adam\")\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the minimum class\n",
    "min_class = min(y_train.unique())\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix from on hot encoded training labels to use to calculate class centroids\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "onehot = onehot_encoder.fit_transform(y_train.reshape((-1, 1)))\n",
    "onehot_inverse = 1/np.sum((onehot.T), axis=1)\n",
    "new_y_train = onehot.T*onehot_inverse.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the training set\n",
    "pred = model.predict(X_train, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply centroid calculation matrix, new_y_train, by training set scores\n",
    "train_cent = np.matmul(new_y_train, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new data model score\n",
    "new_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the closest centroid\n",
    "rcenter = train_cent.T # create row matrix of centroids\n",
    "y_pred = np.argmin(abs(new_pred - rcenter), axis=1) + min_class      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49224806201550386 0.45348837209302323\n"
     ]
    }
   ],
   "source": [
    "# calculate the mean absolute error and mean zero one error\n",
    "mae = np.mean(abs(y_pred - y_test))\n",
    "mze = np.mean(abs(y_pred - y_test) > 0)   \n",
    "print(mae, mze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0],\n",
       "       [ 3, 17,  4,  3,  0,  0],\n",
       "       [ 2, 15, 29, 19,  0,  0],\n",
       "       [ 0,  1,  6, 55, 19,  4],\n",
       "       [ 0,  0,  0, 14, 20, 21],\n",
       "       [ 0,  0,  0,  0,  6, 20]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix \n",
    "confusion_matrix(y_test, y_pred) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
