{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Hyperplane Centroid:\n",
    "\n",
    "$ HC_{k} = {\\bf w}^{T}{\\bf x}- \\frac{1}{ n_{k} } \\sum\\limits_{y_{i}=k} {\\bf w}^{T}{\\bf x_{i}}=0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Hyperplane Centroid Loss:\n",
    "\n",
    "$ HCL = \\sum\\limits_{i=1}^{k-1}=max(HC_{i} -HC_{i+1} + \\delta,0) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Hyperplane Point Loss:\n",
    "\n",
    "(4) $ HPL_{i}^{+}= max(f(x)-HC)-(HC_{+1}-HC)+\\gamma (HC_{+1} - HC),0) $\n",
    "\n",
    "$ =max(f(x_{i})-\\gamma HC - (1-\\gamma)HC_{+1},0)  $ \n",
    "\n",
    "\n",
    "\n",
    "(5)\n",
    "$ HPL_{i}^{-}= max(\\gamma HC - f(x_{i}) + (1-\\gamma)HC_{-1},0)  $\n",
    "\n",
    "(6)\n",
    "$ HPL = \\sum\\limits_{x_{i}\\in S} HPL_{i}^{+} + HPL_{i}^{-}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) $ OHPL = \\alpha HCL + HPL $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Metrics to assess performance on ordinal classification task given class prediction\n",
    "   using hyper plane loss techniques \n",
    "\"\"\"\n",
    "\n",
    "# Authors: Bob Vanderheyden <rvanderh@us.ibm.com>\n",
    "#          Ying Xie <yxie2@kennesaw.edu>\n",
    "#         \n",
    "# Contributor: Shayan Shamskolahi\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def hpall_mean_loss(y_true, y_pred, minlabel, maxlabel, margin=0.1, ordering_loss_weight=1):\n",
    "    \"\"\" Evaluate the ordinal hyperplane ordering loss and point loss of the predictions y_pred\\\n",
    "        (using reduce mean).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : array-like\n",
    "        y_pred : array-like\n",
    "        minlabel : integer\n",
    "        maxlabel : integer\n",
    "        margin : float\n",
    "        ordering_loss_weight : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "        A non-negative floating point value (best value is 0.0)\n",
    "        \n",
    "        Usage\n",
    "        -------\n",
    "        loss = hp_all_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "        print('Loss: ', loss.numpy()) # Loss: 0.7228571\n",
    "        \n",
    "        \n",
    "        Usage with the `compile` API:\n",
    "        \n",
    "        ```python\n",
    "        \n",
    "        Example Keras wrapper for hp_all_loss:\n",
    "        \n",
    "        def get_ohpl_wrapper (min_label, max_label, margin, ordering_loss_weight):\n",
    "            def ohpl(y_true, y_pred):\n",
    "                return hpall_mean_loss(y_true, y_pred, min_label, max_label, margin, ordering_loss_weight)\n",
    "            return ohpl\n",
    "\n",
    "        loss = get_ohpl_wrapper(0,4,.3,0.1)\n",
    "        \n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        model.compile(loss=hp_all_loss, optimizer='adam', loss=ohpl_point_loss)\n",
    "        ```\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    min_label = tf.constant(minlabel, dtype=tf.float32)\n",
    "    max_label = tf.constant(maxlabel, dtype=tf.float32)\n",
    "    margin = tf.constant(margin, dtype=tf.float32) # centroid margin\n",
    "    ordering_loss_weight = tf.constant(ordering_loss_weight, dtype=tf.float32) \n",
    "    \n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.dtypes.cast(y_true, y_pred.dtype)\n",
    "    y_pred = tf.reshape(tf.transpose(y_pred),[-1,1])\n",
    "    \n",
    "    # OHPL ordering loss\n",
    "    # one hot vector for y_true\n",
    "    ords, idx = tf.unique(tf.reshape(y_true, [-1])) \n",
    "    num = tf.shape(ords)[0]\n",
    "    y_true_1hot = tf.one_hot(idx, num)\n",
    "\n",
    "    # mean distance for each class\n",
    "    yO = tf.transpose(y_pred) @ y_true_1hot\n",
    "    yc = tf.reduce_sum(y_true_1hot,0)\n",
    "    class_mean = tf.divide(yO,yc)  \n",
    "\n",
    "    # min. distance\n",
    "    ords = tf.dtypes.cast(ords, tf.float32)\n",
    "    ords0 = tf.reshape(ords, [-1,1])\n",
    "    ords1 = tf.reshape(ords, [1,-1])\n",
    "    \n",
    "    min_distance = tf.subtract(ords0, ords1)\n",
    "    # apply ReLU\n",
    "    min_distance = tf.nn.relu (min_distance)\n",
    "    \n",
    "    # keeps min. distance\n",
    "    keep = tf.minimum(min_distance,1)\n",
    "\n",
    "    # distance to centroid     \n",
    "    class_mean0 = tf.reshape(class_mean, [-1,1])\n",
    "    class_mean1 = tf.reshape(class_mean, [1,-1])\n",
    "    class_mean = tf.subtract(class_mean0, class_mean1)  \n",
    "    # apply ReLU    \n",
    "    class_mean = tf.nn.relu(class_mean)\n",
    "    centroid_distance = tf.multiply(keep, class_mean)\n",
    "    \n",
    "    hp_ordering_loss = tf.subtract(min_distance,centroid_distance)\n",
    "    # apply ReLU\n",
    "    hp_ordering_loss = tf.nn.relu(hp_ordering_loss)\n",
    "    hp_ordering_loss = tf.reduce_sum(hp_ordering_loss)\n",
    "\n",
    "    \n",
    "    # OHPL point loss\n",
    "    # mean distance for each class\n",
    "    yO = tf.transpose(y_pred) @ y_true_1hot\n",
    "    yc = tf.reduce_sum(y_true_1hot,0)\n",
    "    class_mean = tf.divide(yO,yc) \n",
    " \n",
    "    # mean by class\n",
    "    class_mean = tf.reshape(class_mean,[-1,1])\n",
    "    mean_matrix = y_true_1hot @ class_mean\n",
    "    \n",
    "    lower_bound = tf.subtract(min_label,y_true)\n",
    "    lower_bound = tf.add(lower_bound,1)\n",
    "    lower_bound = tf.multiply(lower_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    lower_bound = tf.nn.relu(lower_bound)\n",
    "    lower_bound = tf.add(margin, lower_bound)\n",
    "\n",
    "    upper_bound = tf.subtract(y_true,max_label)\n",
    "    upper_bound = tf.add(upper_bound,1)\n",
    "    upper_bound = tf.multiply(upper_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    upper_bound = tf.nn.relu(upper_bound)\n",
    "    upper_bound = tf.add(margin, upper_bound)    \n",
    "\n",
    "    upper_loss = tf.add(mean_matrix,upper_bound[:,tf.newaxis])\n",
    "    upper_loss = tf.subtract(y_pred,upper_loss)\n",
    "    # apply ReLU    \n",
    "    upper_loss = tf.nn.relu(upper_loss)\n",
    "    \n",
    "    lower_loss = tf.add(lower_bound[:,tf.newaxis],y_pred)\n",
    "    lower_loss = tf.subtract(mean_matrix,lower_loss)\n",
    "    # apply ReLU    \n",
    "    lower_loss = tf.nn.relu(lower_loss)\n",
    "   \n",
    "    hp_point_loss = tf.add(upper_loss, lower_loss)\n",
    "    hp_point_loss = tf.reduce_mean(hp_point_loss)\n",
    "\n",
    "    # aggregate ordering loss and point loss     \n",
    "    mean_loss = tf.add(hp_point_loss,tf.multiply(ordering_loss_weight, (hp_ordering_loss)))\n",
    "    \n",
    "    return mean_loss\n",
    "\n",
    "   \n",
    "    \"\"\"    \n",
    "        References\n",
    "        ----------\n",
    "        .. [1] Vanderheyden, Bob and Ying Xie. Ordinal Hyperplane Loss. (2018). \n",
    "           2018 IEEE International Conference on Big Data (Big Data), \n",
    "           2018 IEEE International Conference On, 2337. https://doi-org.proxy.kennesaw.edu/10.1109/BigData.2018.8622079\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpall_sum_loss(y_true, y_pred, minlabel, maxlabel, margin=0.1, ordering_loss_weight=1):\n",
    "    \"\"\" Evaluate the ordinal hyperplane ordering loss and point loss of the predictions y_pred\\\n",
    "        (using reduce sum).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : array-like\n",
    "        y_pred : array-like\n",
    "        minlabel : integer\n",
    "        maxlabel : integer\n",
    "        margin : float\n",
    "        ordering_loss_weight : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "        A non-negative floating point value (best value is 0.0)\n",
    "        \n",
    "        Usage\n",
    "        -------\n",
    "        loss = hp_all_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "        print('Loss: ', loss.numpy()) # Loss: 3.48\n",
    "        \n",
    "        \n",
    "        Usage with the `compile` API:\n",
    "        \n",
    "        ```python\n",
    "        \n",
    "        Example Keras wrapper for hp_all_loss:\n",
    "        \n",
    "        def get_ohpl_wrapper (min_label, max_label, margin, ordering_loss_weight):\n",
    "            def ohpl(y_true, y_pred):\n",
    "                return hpall_sum_loss(y_true, y_pred, min_label, max_label, margin, ordering_loss_weight)\n",
    "            return ohpl\n",
    "\n",
    "        loss = get_ohpl_wrapper(0,4,.3,0.1)\n",
    "        \n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        model.compile(loss=hp_all_loss, optimizer='adam', loss=ohpl_point_loss)\n",
    "        ```\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    min_label = tf.constant(minlabel, dtype=tf.float32)\n",
    "    max_label = tf.constant(maxlabel, dtype=tf.float32)\n",
    "    margin = tf.constant(margin, dtype=tf.float32) # centroid margin\n",
    "    ordering_loss_weight = tf.constant(ordering_loss_weight, dtype=tf.float32) \n",
    "    \n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.dtypes.cast(y_true, y_pred.dtype)\n",
    "    y_pred = tf.reshape(tf.transpose(y_pred),[-1,1])\n",
    "    \n",
    "    # OHPL ordering loss\n",
    "    # one hot vector for y_true\n",
    "    ords, idx = tf.unique(tf.reshape(y_true, [-1])) \n",
    "    num = tf.shape(ords)[0]\n",
    "    y_true_1hot = tf.one_hot(idx, num)\n",
    "\n",
    "    # mean distance for each class\n",
    "    yO = tf.transpose(y_pred) @ y_true_1hot\n",
    "    yc = tf.reduce_sum(y_true_1hot,0)\n",
    "    class_mean = tf.divide(yO,yc)  \n",
    "\n",
    "    # min. distance\n",
    "    ords = tf.dtypes.cast(ords, tf.float32)\n",
    "    ords0 = tf.reshape(ords, [-1,1])\n",
    "    ords1 = tf.reshape(ords, [1,-1])\n",
    "    \n",
    "    min_distance = tf.subtract(ords0, ords1)\n",
    "    # apply ReLU\n",
    "    min_distance = tf.nn.relu (min_distance)\n",
    "    \n",
    "    # keeps min. distance\n",
    "    keep = tf.minimum(min_distance,1)\n",
    "\n",
    "    # distance to centroid     \n",
    "    class_mean0 = tf.reshape(class_mean, [-1,1])\n",
    "    class_mean1 = tf.reshape(class_mean, [1,-1])\n",
    "    class_mean = tf.subtract(class_mean0, class_mean1)  \n",
    "    # apply ReLU    \n",
    "    class_mean = tf.nn.relu(class_mean)\n",
    "    centroid_distance = tf.multiply(keep, class_mean)\n",
    "    \n",
    "    hp_ordering_loss = tf.subtract(min_distance,centroid_distance)\n",
    "    # apply ReLU\n",
    "    hp_ordering_loss = tf.nn.relu(hp_ordering_loss)\n",
    "    hp_ordering_loss = tf.reduce_sum(hp_ordering_loss)\n",
    "\n",
    "    \n",
    "    # OHPL point loss\n",
    "    # mean distance for each class\n",
    "    yO = tf.transpose(y_pred) @ y_true_1hot\n",
    "    yc = tf.reduce_sum(y_true_1hot,0)\n",
    "    class_mean = tf.divide(yO,yc) \n",
    " \n",
    "    # mean by class\n",
    "    class_mean = tf.reshape(class_mean,[-1,1])\n",
    "    mean_matrix = y_true_1hot @ class_mean\n",
    "    \n",
    "    lower_bound = tf.subtract(min_label,y_true)\n",
    "    lower_bound = tf.add(lower_bound,1)\n",
    "    lower_bound = tf.multiply(lower_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    lower_bound = tf.nn.relu(lower_bound)\n",
    "    lower_bound = tf.add(margin, lower_bound)\n",
    "\n",
    "    upper_bound = tf.subtract(y_true,max_label)\n",
    "    upper_bound = tf.add(lower_bound,1)\n",
    "    upper_bound = tf.multiply(lower_bound,1e9)\n",
    "    # apply ReLU    \n",
    "    upper_bound = tf.nn.relu(lower_bound)\n",
    "    upper_bound = tf.add(margin, lower_bound)    \n",
    "\n",
    "    upper_loss = tf.add(mean_matrix,upper_bound[:,tf.newaxis])\n",
    "    upper_loss = tf.subtract(y_pred,upper_loss)\n",
    "    # apply ReLU    \n",
    "    upper_loss = tf.nn.relu(upper_loss)\n",
    "    \n",
    "    lower_loss = tf.add(lower_bound[:,tf.newaxis],mean_matrix)\n",
    "    lower_loss = tf.subtract(y_pred,lower_loss)\n",
    "    # apply ReLU    \n",
    "    lower_loss = tf.nn.relu(lower_loss)\n",
    "   \n",
    "    hp_point_loss = tf.add(upper_loss, lower_loss)\n",
    "    hp_point_loss = tf.reduce_sum(hp_point_loss)\n",
    "\n",
    "    # aggregate ordering loss and point loss     \n",
    "    sum_loss = tf.add(hp_point_loss,tf.multiply(ordering_loss_weight, hp_ordering_loss))\n",
    "    \n",
    "    return sum_loss\n",
    "\n",
    "\n",
    "    \"\"\"    \n",
    "        References\n",
    "        ----------\n",
    "        .. [1] Vanderheyden, Bob and Ying Xie. Ordinal Hyperplane Loss. (2018). \n",
    "           2018 IEEE International Conference on Big Data (Big Data), \n",
    "           2018 IEEE International Conference On, 2337. https://doi-org.proxy.kennesaw.edu/10.1109/BigData.2018.8622079\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.7228571\n"
     ]
    }
   ],
   "source": [
    "loss = hpall_mean_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "print('Loss: ', loss.numpy()) # Loss: 0.7228571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  3.48\n"
     ]
    }
   ],
   "source": [
    "loss = hpall_sum_loss([4,1,2,0,4,2,1], [6.0,3.1,5.2,1.0,4.0,2.2,3.7],0,4,.3,0.1)\n",
    "print('Loss: ', loss.numpy()) # Loss: 3.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example wrapper for Keras (mean loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example Keras wrapper for hpall_mean_loss\n",
    "\n",
    "def get_ohpl_wrapper (min_label, max_label, margin, ordering_loss_weight):\n",
    "    def ohpl(y_true, y_pred):\n",
    "        return hpall_mean_loss(y_true, y_pred, min_label, max_label, margin, ordering_loss_weight)\n",
    "    return ohpl\n",
    "\n",
    "loss = get_ohpl_wrapper(1,9,.3,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper in action - Keras sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 670 samples, validate on 330 samples\n",
      "Epoch 1/2\n",
      "670/670 [==============================] - 2s 3ms/sample - loss: 0.0872 - accuracy: 0.1388 - val_loss: 0.0818 - val_accuracy: 0.1818\n",
      "Epoch 2/2\n",
      "670/670 [==============================] - 2s 2ms/sample - loss: 0.0820 - accuracy: 0.1806 - val_loss: 0.0818 - val_accuracy: 0.1818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6ecc538b50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "pwd = !pwd\n",
    "df = pd.read_csv(pwd[0]+'/datasets-arie_ben_david-era.csv', header=None, sep = ',')\n",
    "\n",
    "X = df.iloc[:,:4]\n",
    "y = df.iloc[:,4]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(40, activation='relu', input_shape=(4, )))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(28, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=loss, optimizer=\"adam\")\n",
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=2, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example wrapper for Keras (sum loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example Keras wrapper for hpall_sum_loss\n",
    "\n",
    "def get_ohpl_wrapper (min_label, max_label, margin, ordering_loss_weight):\n",
    "    def ohpl(y_true, y_pred):\n",
    "        return hpall_sum_loss(y_true, y_pred, min_label, max_label, margin, ordering_loss_weight)\n",
    "    return ohpl\n",
    "\n",
    "loss = get_ohpl_wrapper(1,9,.3,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper in action - Keras sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 670 samples, validate on 330 samples\n",
      "Epoch 1/2\n",
      "670/670 [==============================] - 2s 3ms/sample - loss: 0.0859 - accuracy: 0.1657 - val_loss: 0.0830 - val_accuracy: 0.1758\n",
      "Epoch 2/2\n",
      "670/670 [==============================] - 2s 2ms/sample - loss: 0.0823 - accuracy: 0.1851 - val_loss: 0.0853 - val_accuracy: 0.1455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6ec6c58d90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "pwd = !pwd\n",
    "df = pd.read_csv(pwd[0]+'/datasets-arie_ben_david-era.csv', header=None, sep = ',')\n",
    "\n",
    "X = df.iloc[:,:4]\n",
    "y = df.iloc[:,4]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(40, activation='relu', input_shape=(4, )))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(28, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=loss, optimizer=\"adam\")\n",
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=2, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Keras implementation using subclassing for mean loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class OHPLall_mean(keras.losses.Loss):\n",
    "    def __init__(self, minlabel, maxlabel, margin, ordering_loss_weight, **kwargs):\n",
    "        self.minlabel = minlabel\n",
    "        self.maxlabel = maxlabel\n",
    "        self.margin = margin\n",
    "        self.ordering_loss_weight = ordering_loss_weight\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        min_label = tf.constant(self.minlabel, dtype=tf.float32)\n",
    "        max_label = tf.constant(self.maxlabel, dtype=tf.float32)\n",
    "        margin = tf.constant(self.margin, dtype=tf.float32) # centroid margin\n",
    "        ordering_loss_weight = tf.constant(self.ordering_loss_weight, dtype=tf.float32) \n",
    "\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        y_true = tf.dtypes.cast(y_true, y_pred.dtype)\n",
    "        y_pred = tf.reshape(tf.transpose(y_pred),[-1,1])\n",
    "\n",
    "        # OHPL ordering loss\n",
    "        # one hot vector for y_true\n",
    "        ords, idx = tf.unique(tf.reshape(y_true, [-1])) \n",
    "        num = tf.shape(ords)[0]\n",
    "        y_true_1hot = tf.one_hot(idx, num)\n",
    "\n",
    "        # mean distance for each class\n",
    "        yO = tf.transpose(y_pred) @ y_true_1hot\n",
    "        yc = tf.reduce_sum(y_true_1hot,0)\n",
    "        class_mean = tf.divide(yO,yc)  \n",
    "\n",
    "        # min. distance\n",
    "        ords = tf.dtypes.cast(ords, tf.float32)\n",
    "        ords0 = tf.reshape(ords, [-1,1])\n",
    "        ords1 = tf.reshape(ords, [1,-1])\n",
    "\n",
    "        min_distance = tf.subtract(ords0, ords1)\n",
    "        # apply ReLU\n",
    "        min_distance = tf.nn.relu (min_distance)\n",
    "\n",
    "        # keeps min. distance\n",
    "        keep = tf.minimum(min_distance,1)\n",
    "\n",
    "        # distance to centroid     \n",
    "        class_mean0 = tf.reshape(class_mean, [-1,1])\n",
    "        class_mean1 = tf.reshape(class_mean, [1,-1])\n",
    "        class_mean = tf.subtract(class_mean0, class_mean1)  \n",
    "        # apply ReLU    \n",
    "        class_mean = tf.nn.relu(class_mean)\n",
    "        centroid_distance = tf.multiply(keep, class_mean)\n",
    "\n",
    "        hp_ordering_loss = tf.subtract(min_distance,centroid_distance)\n",
    "        # apply ReLU\n",
    "        hp_ordering_loss = tf.nn.relu(hp_ordering_loss)\n",
    "        hp_ordering_loss = tf.reduce_sum(hp_ordering_loss)\n",
    "\n",
    "\n",
    "        # OHPL point loss\n",
    "        # mean distance for each class\n",
    "        yO = tf.transpose(y_pred) @ y_true_1hot\n",
    "        yc = tf.reduce_sum(y_true_1hot,0)\n",
    "        class_mean = tf.divide(yO,yc) \n",
    "\n",
    "        # mean by class\n",
    "        class_mean = tf.reshape(class_mean,[-1,1])\n",
    "        mean_matrix = y_true_1hot @ class_mean\n",
    "\n",
    "        lower_bound = tf.subtract(min_label,y_true)\n",
    "        lower_bound = tf.add(lower_bound,1)\n",
    "        lower_bound = tf.multiply(lower_bound,1e9)\n",
    "        # apply ReLU    \n",
    "        lower_bound = tf.nn.relu(lower_bound)\n",
    "        lower_bound = tf.add(margin, lower_bound)\n",
    "\n",
    "        upper_bound = tf.subtract(y_true,max_label)\n",
    "        upper_bound = tf.add(upper_bound,1)\n",
    "        upper_bound = tf.multiply(upper_bound,1e9)\n",
    "        # apply ReLU    \n",
    "        upper_bound = tf.nn.relu(upper_bound)\n",
    "        upper_bound = tf.add(margin, upper_bound)    \n",
    "\n",
    "        upper_loss = tf.add(mean_matrix,upper_bound[:,tf.newaxis])\n",
    "        upper_loss = tf.subtract(y_pred,upper_loss)\n",
    "        # apply ReLU    \n",
    "        upper_loss = tf.nn.relu(upper_loss)\n",
    "\n",
    "        lower_loss = tf.add(lower_bound[:,tf.newaxis],y_pred)\n",
    "        lower_loss = tf.subtract(mean_matrix,lower_loss)\n",
    "        # apply ReLU    \n",
    "        lower_loss = tf.nn.relu(lower_loss)\n",
    "\n",
    "        hp_point_loss = tf.add(upper_loss, lower_loss)\n",
    "        hp_point_loss = tf.reduce_mean(hp_point_loss) # for sum loss, replace with tf.reduce_sum()\n",
    "\n",
    "        # aggregate ordering loss and point loss     \n",
    "        mean_loss = tf.add(hp_point_loss,tf.multiply(ordering_loss_weight, (hp_ordering_loss)))\n",
    "\n",
    "        return mean_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subclassing in action - Keras sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 670 samples, validate on 330 samples\n",
      "Epoch 1/2\n",
      "670/670 [==============================] - 2s 3ms/sample - loss: 0.9285 - accuracy: 0.1418 - val_loss: 0.8966 - val_accuracy: 0.1758\n",
      "Epoch 2/2\n",
      "670/670 [==============================] - 2s 2ms/sample - loss: 0.8960 - accuracy: 0.1761 - val_loss: 0.8930 - val_accuracy: 0.1667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2773713b90>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "pwd = !pwd\n",
    "df = pd.read_csv(pwd[0]+'/datasets-arie_ben_david-era.csv', header=None, sep = ',')\n",
    "\n",
    "X = df.iloc[:,:4]\n",
    "y = df.iloc[:,4]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(40, activation='relu', input_shape=(4, )))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(28, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=OHPLall_mean(1,9,0.1,1), optimizer=\"adam\")\n",
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=2, batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
